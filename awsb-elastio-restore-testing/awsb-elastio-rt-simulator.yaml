AWSTemplateFormatVersion: '2010-09-09'
Description: CloudFormation template to deploy the Helper for Elastio Integration

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: Elastio Integration Configuration
        Parameters: 
            - LogsBucketName  
            - ElastioScanTag
            - RPForwardTag          
            - ElastioScanAfterCopyDestination 
            - EnableSHIntegration
            - EnableObserverIntegration  
            - EnableExtendedTagging         
            - ElastioTagKey
            - ElastioTagValue
            - ElastioDefaultVaultName            
    ParameterLabels:
        LogsBucketName:
            default: S3 Bucket for Elastio Logs and Data 
        ElastioScanTag:
            default: RecoveryPoint Tag to initiate Elastio Scan
        RPForwardTag:
            default: RecoveryPoint Tag to initiate AWS Backup Copy       
        ElastioScanAfterCopyDestination:
            default: AWS Backup Vault for post-scan copy
        EnableSHIntegration:
            default: Generate SecurityHub events for anomalies
        EnableObserverIntegration:
            default: Send events to Observer for reporting
        EnableExtendedTagging:
            default: Enable extended tagging for RecoveryPoints
        ElastioTagKey:
            default: Tag key for EventBus access control
        ElastioTagValue:
            default: Tag value for EventBus access control
        ElastioDefaultVaultName:
            default: Default Elastio Vault Name

Parameters:
  LogsBucketName:
    Description: The Name of the S3 Bucket where the Scan Logs and Reports are to be stored. 
    Type: String
  ElastioScanTag:
    Description: The Tag in an AWS Backup RecoveryPoint that will initiate an Elastio Scan. Supports values scan, ingest, ingest-and-scan
    Type: String
    Default: 'elastio:restore-test'
  RPForwardTag:
    Description: The Tag in an AWS Backup RecoveryPoint that will initiate an AWS Backup Copy to another Vault mentioned in ElastioScanAfterCopyDestination
    Type: String
    Default: 'lag:fwd' 
  ElastioSupportedRTForRestoreSimulation: 
    Description: The Resource Type supported for AWS Backup Restore Testing Simulation
    Type: String
    Default: 'VirtualMachine'   
  ElastioScanAfterCopyDestination:
    Description: The Arn of the AWS Backup Vault to copy after a successfull Elastio Scan
    Type: String
  ElastioTagKey:
    Type: String
    Description: 'Tag key set to elastio resources for EB Permission.[DO NOT MODIFY Unless Advised]'
    Default: 'elastio:iscan-event-bus'
  ElastioTagValue:
    Type: String
    Description: 'Tag value set to elastio resources for EB Permission.[DO NOT MODIFY Unless Advised]'
    Default: 'true' 
  ElastioDefaultVaultName:
    Type: String
    Description: 'The Name of the Elastio Default Vault Name.[DO NOT MODIFY Unless Advised]'
    Default: 'default'   
  RestoreSimulationInstanceType:
    Type: String
    Description: 'The instance type for the Restore Simulated Virtual Machine.[DO NOT MODIFY Unless Advised]'
    Default: 't1.micro' 
  EnableSHIntegration:
    Type: String
    Default: False
    AllowedValues:
      - True
      - False
    Description: SecurityHub should be enabled.Refer to https://aws.amazon.com/security-hub/
  EnableObserverIntegration:
    Type: String
    Default: False
    AllowedValues:
      - True
      - False
    Description: Observer should be configured.Refer to https://aws.amazon.com/blogs/storage/obtain-aggregated-daily-cross-account-multi-region-aws-backup-reporting/       
  EnableExtendedTagging:
    Type: String
    Default: True
    AllowedValues:
      - True
      - False
    Description: 'This will add additional meta data to AWSBackup RPs starting with awsb:original-asset:'
Resources: 

  # Restore Simulator Lambda Function
  ElastioHelperLambda:
    Type: "AWS::Lambda::Function"
    Properties:
      FunctionName: "ElastioHelperLambda"
      Handler: "index.lambda_handler"
      Role: !GetAtt ElastioHelperLambdaRole.Arn
      Environment:
        Variables:
          ElastioStatusEB : !Ref ElastioHelperEventBus 
          LogsBucketName: !Ref LogsBucketName
          ElastioImportLambdaARN : !Sub "arn:${AWS::Partition}:lambda:${AWS::Region}:${AWS::AccountId}:function:elastio-bg-jobs-service-aws-backup-integration"
          ElastioScanTag: !Ref ElastioScanTag
          EnableExtendedTagging: !Ref EnableExtendedTagging
          RPForwardTag: !Ref RPForwardTag
          EnableSHIntegration: !Ref EnableSHIntegration
          EnableObserverIntegration: !Ref EnableObserverIntegration          
          ElastioScanAfterCopyDestination: !Ref ElastioScanAfterCopyDestination
          AWSBackupCopyIamRole: !Sub "arn:${AWS::Partition}:iam::${AWS::AccountId}:role/service-role/AWSBackupDefaultServiceRole"
          AWSBackupEventsBusArn: !Sub "arn:${AWS::Partition}:events:${AWS::Region}:${AWS::AccountId}:event-bus/default"      
          ElastioSupportedRTForRestoreSimulation: !Ref ElastioSupportedRTForRestoreSimulation
          RestoreSimulationInstanceType: !Ref RestoreSimulationInstanceType
          ElastioDefaultVaultName: !Ref ElastioDefaultVaultName
      Runtime: python3.12
      Architectures: 
            - arm64
      Timeout: 900              
      Code:
        ZipFile: |
            # pylint: disable = C0103, C0116,C0303,R0902, W1203, C0301, W0311, W0718 , W1514, W0719
            #!/usr/bin/env python
            # -*- coding: utf-8 -*-
            import os
            from datetime import timezone
            import datetime
            import logging
            import traceback
            import json
            import uuid
            import boto3
            from botocore.exceptions import ClientError

        

            logger = logging.getLogger()
            logger.setLevel(logging.INFO)
            sechub_client = boto3.client('securityhub')
            ec2_client = boto3.client("ec2")
            sts_client = boto3.client("sts")
            backup_client = boto3.client('backup')   
            events_client = boto3.client("events")
            efs_client = boto3.client('efs')

            # Define a dictionary for resource type handling
            resource_handlers = {
                'ec2': ec2_client.create_tags,
                'ebs': ec2_client.create_tags,
                'backup': backup_client.tag_resource
            }            
            # Define constants
            ALLOWED_RESOURCE_TYPES = {"ec2"}    
            RT_STATUS_MESSAGE_MAX_LENGTH=1000
            INTEGRATION_DEFAULT_SG_NAME = 'ElastioSGForDefaultVPCAccess'
            INTEGRATION_DEFAULT_SG_DESCRIPTION = 'Allow all traffic to / from ElastioSG for default vault'

            # Environment variables
            ELASTIO_DEFAULT_VAULT_NAME = os.environ.get('ElastioDefaultVaultName', 'default')
            ELASTIO_SOURCE_SG = f'elastio-vault-{ELASTIO_DEFAULT_VAULT_NAME}-batch-security-group'
            ENABLE_ELASTIO_SCAN_TAG = os.environ.get('ElastioScanTag', 'elastio:restore-test')
            RP_FWD_TAG = os.environ.get('RPForwardTag', 'rp:fwd')  
            ELASTIO_SCAN_AFTER_COPY_DESTINATION = os.environ.get('ElastioScanAfterCopyDestination')
            AWSB_COPY_IAM_ROLE = os.environ.get('AWSBackupCopyIamRole')


            # Log configuration information
            logger.info(f'Elastio Default Vault Name: {ELASTIO_DEFAULT_VAULT_NAME}')
            logger.info(f'Elastio Scan Tag: {ENABLE_ELASTIO_SCAN_TAG}')
            logger.info(f'RP Forward Tag: {RP_FWD_TAG}')
            logger.info(f'Copy Destination: {ELASTIO_SCAN_AFTER_COPY_DESTINATION}')
            logger.info(f'IAM Role specified for Copy Operation: {AWSB_COPY_IAM_ROLE}')

            def get_default_vpc_id():
                try:
                    ec2_client = boto3.client('ec2')
                    response = ec2_client.describe_vpcs(Filters=[{'Name': 'isDefault', 'Values': ['true']}])
                    default_vpc_id = response['Vpcs'][0]['VpcId']
                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} in get_default_vpc_id")
                return default_vpc_id         

            def tag_recovery_point_with_meta_data(resource_type, recovery_point_arn):
                """
                This method is responsible for tagging an EC2 RP with the constituent EBS Volume Ids
                """    
                resource_id, partition, region, account_id = get_resource_id_from_arn(recovery_point_arn)
                logger.info(f'tag_recovery_point_with_meta_data for {partition}:{account_id}:{region}:{resource_type}:{resource_id} from {recovery_point_arn}')
                
                resource_type = resource_type.lower()
                new_tag_list = {
                    "awsb:original-asset:account-id": account_id,
                    "awsb:original-asset:region": region,
                    "awsb:original-asset:rp-rn": recovery_point_arn
                }              
                
                try:
                    if resource_type in ALLOWED_RESOURCE_TYPES:        
                        # Extract backing snapshots from AMI Id
                        describe_images_info = ec2_client.describe_images(ImageIds=[resource_id])
                        if "ResponseMetadata" in describe_images_info:
                            del describe_images_info["ResponseMetadata"]

                        for image in describe_images_info.get("Images", []):
                            new_tag_list.update({
                                f"awsb:original-asset:vol:{block_dev_mapping.get('DeviceName')}": get_volume_id_from_snapshot(block_dev_mapping.get("Ebs", {}).get("SnapshotId"))
                                for block_dev_mapping in image.get("BlockDeviceMappings", [])
                                if "Ebs" in block_dev_mapping
                            })

                    # Set the tags on resources
                    set_tags_by_resource(resource_type, resource_id, recovery_point_arn, new_tag_list)                        
                except Exception:
                    logger.error(f"Error {traceback.format_exc()} processing tag_recovery_point_with_meta_data for resource_id : {resource_id}")


            def get_volume_id_from_snapshot(snapshot_id):
                """
                This method is responsible for getting the volume id from the given snapshot id
                """
                try:
                    response = ec2_client.describe_snapshots(SnapshotIds=[snapshot_id])
                    snapshots = response.get('Snapshots', [])
                    
                    if snapshots:
                        return snapshots[0].get('VolumeId')
                    else:
                        logger.warning(f"No snapshot found with ID: {snapshot_id}")
                        return None
                except ClientError as e:
                    logger.error(f"Error describing snapshot {snapshot_id}: {e}")
                    return None
                except Exception as e:
                    logger.error(f"Unexpected error getting volume ID for snapshot {snapshot_id}: {e}")
                    return None


            def get_resource_id_from_arn(recovery_point_arn):
                """
                Helper function to get the ResourceId and other details from a given Recovery Point Arn.
                """
                try:
                    # Split the ARN by ':'
                    parts = recovery_point_arn.split(':')
                    print(parts)
                    
                    # Check if the ARN is properly formatted
                    if len(parts) < 6:
                        raise ValueError("Invalid ARN format")
                    
                    # Create a dictionary for easy access to ARN parts
                    arn_parts = {
                        'partition': parts[1],
                        'region': parts[3],
                        'account_id': parts[4],
                        'resource': ':'.join(parts[5:])
                    }
                    
                    # Determine the resource_id based on ARN format
                    if '/' in arn_parts['resource']:
                        # Handle for EC2 and EBS
                        resource_id = arn_parts['resource'].split('/')[-1]
                    elif ':::' in arn_parts['resource']:
                        # Handle for S3
                        resource_id = arn_parts['resource'].split(':::')[-1]
                    else:
                        # Default case
                        resource_id = arn_parts['resource'].split(':')[-1]
                    # Get account ID if not present in ARN
                    if not arn_parts['account_id']:
                        arn_parts['account_id'] = sts_client.get_caller_identity()["Account"]                        
                    
                    return resource_id, arn_parts['partition'], arn_parts['region'], arn_parts['account_id']
                
                except ValueError as ve:
                    logger.error(f"Error parsing ARN: {ve}")
                    raise
                except ClientError as ce:
                    logger.error(f"Error getting caller identity: {ce}")
                    raise
                except Exception as e:
                    logger.error(f"Unexpected error in get_resource_id_from_arn: {e}")
                    raise

            def set_tags_by_resource(resource_type, resource_id, recovery_point_arn, new_tag_list):
                """
                Helper function to set the tags on a given resource. 
                """
                logger.info(f"Starting set_tags_by_resource {recovery_point_arn} with new tag list: {new_tag_list}")
                resource_type = resource_type.lower()

                try:
                    if resource_type in ('ec2', 'ebs'):
                        tags_to_apply = [{'Key': key, 'Value': value} for key, value in new_tag_list.items() if not key.startswith('aws:')]
                        logger.info(f"Applying {tags_to_apply} to {resource_id}")
                        resource_handlers[resource_type](Resources=[resource_id], Tags=tags_to_apply)
                    
                    elif ':backup:' in recovery_point_arn:
                        logger.info(f"Applying {new_tag_list} to {recovery_point_arn}")
                        resource_handlers['backup'](ResourceArn=recovery_point_arn, Tags=new_tag_list)
                    
                    else:
                        logger.info(f"{resource_type} Not Supported Yet. Skipping Processing")
                
                except ClientError as e:
                    logger.error(f"AWS API error while setting tags for {resource_type} {resource_id}: {e}")
                except Exception as e:
                    logger.error(f"Unexpected error while setting tags for {resource_type} {resource_id}: {e}")


            def create_security_group(group_name, vpc_id, description):
                """
                Create a security group in the specified VPC.
                
                :param group_name: Name of the security group
                :param vpc_id: ID of the VPC where the security group will be created
                :param description: Description of the security group
                :return: ID of the created security group
                """
                # Input validation
                if not all([group_name, vpc_id, description]):
                    raise ValueError("All parameters (group_name, vpc_id, description) must be provided")

                try:
                    # Create the security group
                    response = ec2_client.create_security_group(
                        GroupName=group_name,
                        Description=description,
                        VpcId=vpc_id
                    )

                    security_group_id = response['GroupId']
                    logger.info(f"Security Group created with ID: {security_group_id}")

                    return security_group_id

                except ClientError as e:
                    error_code = e.response['Error']['Code']
                    error_message = e.response['Error']['Message']
                    logger.error(f"Failed to create security group. Error: {error_code} - {error_message}")
                    raise

                except Exception as e:
                    logger.error(f"Unexpected error occurred while creating security group: {str(e)}")
                    raise

            def allow_all_traffic_from_elastio_sg(security_group_id):
                """
                Allow all inbound traffic from the Elastio source security group to the specified security group.
                
                :param security_group_id: ID of the security group to modify
                """
                # Input validation
                if not security_group_id:
                    raise ValueError("security_group_id must be provided")

                try:
                    # Authorize inbound traffic from Elastio security group
                    ec2_client.authorize_security_group_ingress(
                        GroupId=security_group_id,
                        IpPermissions=[
                            {
                                'IpProtocol': '-1',  # Consider restricting this to specific protocols/ports
                                'UserIdGroupPairs': [{'GroupName': ELASTIO_SOURCE_SG}]
                            }
                        ]
                    )
                    logger.info(f"Successfully allowed all traffic from {ELASTIO_SOURCE_SG} to security group {security_group_id}")

                except ClientError as e:
                    error_code = e.response['Error']['Code']
                    error_message = e.response['Error']['Message']
                    logger.error(f"Failed to authorize ingress for security group {security_group_id}. Error: {error_code} - {error_message}")
                    raise

                except Exception as e:
                    logger.error(f"Unexpected error occurred while authorizing ingress for security group {security_group_id}: {str(e)}")
                    logger.error(f"Traceback: {traceback.format_exc()}")
                    raise

            def attach_elastio_sg_to_default_sg():

                try:
                    ec2_client = boto3.client('ec2')
                    security_group_id = None
                    response = ec2_client.describe_security_groups(Filters=[{'Name': 'group-name', 'Values': [INTEGRATION_DEFAULT_SG_NAME]}])
                    security_groups = response.get('SecurityGroups')
                    
                    if not security_groups or len(security_groups) < 1:
                        response = ec2_client.describe_vpcs(Filters=[{'Name': 'isDefault', 'Values': ['true']}])
                        default_vpc_id = response['Vpcs'][0]['VpcId']

                        # Create the security group
                        security_group_id = create_security_group(INTEGRATION_DEFAULT_SG_NAME, default_vpc_id, INTEGRATION_DEFAULT_SG_DESCRIPTION)

                        # Allow all traffic from existing security groups
                        allow_all_traffic_from_elastio_sg(security_group_id)

                    else:
                        security_group_id = response['SecurityGroups'][0]['GroupId']
                        logger.info(f'ELASTIO_SOURCE_SG : {INTEGRATION_DEFAULT_SG_NAME} already exists. NO Action done. ')
                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} managing SGs in attach_elastio_sg_to_default_sg")  

                return security_group_id 

            def get_default_vpc_subnets():

                subnets = None
                try:
                    ec2_client = boto3.client('ec2')
                    default_vpc_id = get_default_vpc_id()
                    subnets = ec2_client.describe_subnets(Filters=[{'Name': 'vpc-id', 'Values': [default_vpc_id]}])['Subnets']
                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} finding default subnets in get_default_vpc_subnets")
                return subnets

            def create_efs_mount_targets(efs_file_system_id, subnets, security_group_id):
                try:
                    if subnets:
                        logger.info(f"create_efs_mount_targets for subnet {subnets} with SG : {security_group_id} for FS : {efs_file_system_id}")
                        for subnet in subnets:
                            subnet_id = subnet['SubnetId']
                            response = efs_client.create_mount_target(
                                FileSystemId=efs_file_system_id,
                                SubnetId=subnet_id,
                                SecurityGroups=[security_group_id]
                            )

                            logger.info(f"Mount target created for subnet {subnet_id}: {response['MountTargetId']}")
                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} in create_efs_mount_targets")   

            def get_efs_resource_id(efs_arn):
                # Split the ARN by colon (':') and get the last part
                # which is the resource ID (e.g., 'fs-12345678')
                parts = efs_arn.split('file-system/')
                resource_id = parts[-1]

                return resource_id

            def find_aws_backup_vault_name(recovery_point_arn):
                rp_backup_vault_name = None
                logger.info(f'Starting expensive search of Backup Vault name using ARN : {recovery_point_arn}')
                try:

                    backup_client = boto3.client('backup')

                    all_backup_vaults =[]
                    # List all available backup vaults
                    response = backup_client.list_backup_vaults()
                    all_backup_vaults = response["BackupVaultList"]
                    while "NextToken" in response:
                        response = backup_client.list_backup_vaults(NextToken=response["NextToken"])
                        all_backup_vaults.extend(response["BackupVaultList"])      


                    # List all available LAG vaults
                    response = backup_client.list_backup_vaults(ByVaultType='LOGICALLY_AIR_GAPPED_BACKUP_VAULT',ByShared=True)
                    all_backup_vaults.extend(response["BackupVaultList"])
                    while "NextToken" in response:
                        response = backup_client.list_backup_vaults(ByVaultType='LOGICALLY_AIR_GAPPED_BACKUP_VAULT',ByShared=True,NextToken=response["NextToken"])
                        all_backup_vaults.extend(response["BackupVaultList"])      

                    logger.info(f'all_backup_vaults : {all_backup_vaults}')

                    all_recovery_points = []
                    # Search for the target ARN across vaults
                    for backup_vault in all_backup_vaults:
                        backup_vault_name = backup_vault['BackupVaultName']
                        backup_vault_arn = backup_vault['BackupVaultArn']
                        
                        parts = backup_vault_arn.split(":")
                        backup_vault_account_id = parts[4] if len(parts) >= 5 else None

                        logger.info(f'looking for {recovery_point_arn} in {backup_vault_name}')
                        # List recovery points in the current vault
                        response = backup_client.list_recovery_points_by_backup_vault(BackupVaultName=backup_vault_name,BackupVaultAccountId=backup_vault_account_id)
                        all_recovery_points = response["RecoveryPoints"]
                        while "NextToken" in response:
                            response = backup_client.list_recovery_points_by_backup_vault(BackupVaultName=backup_vault_name,BackupVaultAccountId=backup_vault_account_id,NextToken=response["NextToken"])
                            all_recovery_points.extend(response["RecoveryPoints"])  


                        # Check if the target ARN is in the current vault
                        for recovery_point in all_recovery_points:
                            if recovery_point['RecoveryPointArn'] == recovery_point_arn:
                                logger.info(f'Matched recovery_point : {recovery_point} for ARN : {recovery_point_arn}')
                                rp_backup_vault_name = recovery_point['BackupVaultName']
                                break
                        if rp_backup_vault_name:
                            break

                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} searching for recovery point ARN")        
                
                return rp_backup_vault_name

            def prep_for_efs_restore_test(restored_resource_id):
                logger.info(f'Processing prep_for_efs_restore_test for :{restored_resource_id}')
                #Create and Attach the SGs as required
                security_group_id = attach_elastio_sg_to_default_sg()

                # Get subnets in the default VPC to prepare mount targets
                default_vpc_subnets = get_default_vpc_subnets()

                if default_vpc_subnets:
                    # Create EFS mount targets in the default VPC subnets
                    create_efs_mount_targets(restored_resource_id, default_vpc_subnets, security_group_id) 
                else:
                    logger.error("Error getting default_vpc_subnets. Elastio test will fail")


            def handle_backup_event(event):
                """
                This method is responsible for processing the AWS Backup Events to initiate 
                an elastio scan
                """
                try:
                    detail_type = event.get("detail-type")
                    event_detail = event.get("detail")
                    job_event_state = event_detail.get("state")
                    resources = event.get('resources')
                    if not job_event_state:
                        # Hack
                        job_event_state = event_detail.get("status")        
                    recovery_point_arn = None
                    for resource in resources:
                        if ':backup-vault:' not in resource:
                            recovery_point_arn = resource
                    tag_list = None
                    if job_event_state != 'COMPLETED':
                        logger.info(f'Combination of detail_type : {detail_type} and job_event_state :{job_event_state} NOT handled !!')
                        return
                    else:
                        backup_client = boto3.client('backup')
                        response = backup_client.list_tags(ResourceArn=recovery_point_arn)
                        tag_list = response.get('Tags')            
                    logger.info(f'tag_list : {tag_list}')
                    SUPPORTED_TYPE_FOR_SIMULATION = os.environ.get('ElastioSupportedRTForRestoreSimulation')
                    if not SUPPORTED_TYPE_FOR_SIMULATION:
                        logger.info('NO supported ResourceType defined for RestoreSimulation. Defaulting to VirtualMachine')
                        SUPPORTED_TYPE_FOR_SIMULATION = 'VirtualMachine'

                    if detail_type == "Recovery Point State Change" and job_event_state =="COMPLETED":
                        logger.info(f'Handling AWS Backup event for :{job_event_state} and {detail_type} for recovery_point_arn:{recovery_point_arn}') 
                        backup_vault_name = event_detail.get('backupVaultName')
                        iam_role_arn=event_detail.get('iamRoleArn')
                        if recovery_point_arn:                
                            response = backup_client.describe_recovery_point(
                                BackupVaultName=backup_vault_name,
                                RecoveryPointArn=recovery_point_arn)
                            resource_type = response.get('ResourceType')
                            enable_extended_tagging = os.environ.get('EnableExtendedTagging')
                            if enable_extended_tagging and enable_extended_tagging.lower() in ['true', '1', 't', 'y','yes']:
                                logger.info(f'Extended Tagging is enabled for Arn : {recovery_point_arn}')
                                tag_recovery_point_with_meta_data(resource_type,recovery_point_arn)
                            else:
                                logger.info(f'Extended Tagging is Disabled for Arn : {recovery_point_arn}')   

                            enable_elastio_scan = ENABLE_ELASTIO_SCAN_TAG in tag_list
                            enable_rp_fwd = RP_FWD_TAG in tag_list
                            if resource_type == SUPPORTED_TYPE_FOR_SIMULATION and enable_elastio_scan:
                                logger.info(f'{recovery_point_arn} of type : {resource_type} is enabled for Elastio Scan')
                                response = backup_client.get_recovery_point_restore_metadata(BackupVaultName=backup_vault_name,RecoveryPointArn=recovery_point_arn)
                                restore_meta_data = response.get('RestoreMetadata')
                                if restore_meta_data:
                                    RS_INSTANCE_TYPE = os.environ.get('RestoreSimulationInstanceType')
                                    if not RS_INSTANCE_TYPE:
                                        logger.info('NO supported RestoreSimulationInstanceType defined. Defaulting to t1.micro')
                                        RS_INSTANCE_TYPE = 't1.micro'

                                    default_vpc_id = get_default_vpc_id()
                                    if not default_vpc_id:
                                        raise Exception('Default VPC missing!')
                                    subnet_ids = get_default_vpc_subnets()
                                    if len(subnet_ids) > 0:
                                        default_subnet_id = subnet_ids[0]['SubnetId']
                                    else:
                                        raise Exception('Not Default Subnets Exist. Stopping Restore Simulation!')
                                    #Create and Attach the SGs as required
                                    attach_elastio_sg_to_default_sg()                        
                                    logger.info(f'Processing Default VPC : {default_vpc_id} with Subnet : {default_subnet_id}')
                                    #https://docs.aws.amazon.com/aws-backup/latest/devguide/restoring-vm.html
                                    meta_data = {
                                        "EbsOptimized": "false",
                                        "InstanceInitiatedShutdownBehavior": "stop",
                                        "InstanceType": RS_INSTANCE_TYPE ,
                                        "RestoreTo": "EC2Instance",
                                        "SubnetId": default_subnet_id,
                                        "VpcId": default_vpc_id
                                    }

                                    logger.info(f'Launching Restore Job for : {resource_type}:{recovery_point_arn} with meta_data :{meta_data} using :{iam_role_arn}')
                                    response = backup_client.start_restore_job(
                                        RecoveryPointArn=recovery_point_arn,
                                        IamRoleArn=iam_role_arn,
                                        ResourceType=resource_type,
                                        Metadata=meta_data
                                    )
                                    restore_job_id = response['RestoreJobId']
                                    logger.info(f'Launched AWS Backup Restore Job with restore_job_id :{restore_job_id}')

                                    rt_workflow_tag = {
                                            "awsb:rt-simulator:restore-job-id": restore_job_id
                                        } 
                                    #Apply a tracking tag to this RT simulator job
                                    backup_client = boto3.client('backup')
                                    logger.info(f"Applying {rt_workflow_tag} to {recovery_point_arn} for handling RT Simulation")
                                    backup_client.tag_resource(ResourceArn=recovery_point_arn, Tags=rt_workflow_tag)                                      
                                else:
                                    logger.error(f'Restore Metadata is missing. Skipping Restore Test Simulation for {backup_vault_name}:{recovery_point_arn}')
                            else:
                                logger.info(f'ResourceType :{resource_type} in {backup_vault_name}:{recovery_point_arn} not a Restore Simulation enabled, Current List : {SUPPORTED_TYPE_FOR_SIMULATION}.')
                    #Handle only COMPLETED Restore Job completion events
                    elif detail_type == "Restore Job State Change":
                        restore_job_id = event_detail.get('restoreJobId')
                        logger.info(f'Processing restore_job_id :{restore_job_id}')
                        rt_plan_arn = event_detail.get('restoreTestingPlanArn')
                        if rt_plan_arn:
                            logger.info(f'restore_job_id :{restore_job_id} part of rt_plan : {rt_plan_arn}')
                        else:
                            logger.info(f'restore_job_id :{restore_job_id} part NOT part of any rt_plan')
                            rt_plan_arn=f'on-demand-{restore_job_id}'

                        created_resource_arn = event_detail.get('createdResourceArn')
                        resource_type = event_detail.get('resourceType')
                        job_event_state = event_detail.get('status')
                        logger.info(f'Processing :{resource_type}:{created_resource_arn} under job_event_state :{job_event_state}' )

                        recovery_point_arn = None
                        for resource in resources:
                            if ':backup-vault:' not in resource:
                                recovery_point_arn = resource

                        backup_client = boto3.client('backup')
                        response = backup_client.list_tags(ResourceArn=recovery_point_arn)
                        tag_list = response.get('Tags')
                        enable_elastio_scan = ENABLE_ELASTIO_SCAN_TAG in tag_list
                        enable_rp_fwd = RP_FWD_TAG in tag_list 
                        restore_job_id = event_detail.get('restoreJobId')

                        #Existance of Scan enable or the Lambda trigger tag
                        if enable_elastio_scan:
                            backup_vault_name = find_aws_backup_vault_name(recovery_point_arn)
                            logger.info(f"Elastio scanning enabled for {backup_vault_name}:{recovery_point_arn} via tag_list : {tag_list}")
                            elastio_status_eb = os.environ.get('ElastioStatusEB')
                            if not elastio_status_eb:
                                logger.info('ElastioStatusEB env variable NOT defined. Defaulting to elastio-scan-results.')
                                elastio_status_eb = 'elastio-scan-results'
                            
                            elastio_lambda_arn = os.environ.get('ElastioImportLambdaARN')
                            if not elastio_lambda_arn:
                                LAMBDA_TRIGGER_TAG = os.environ.get("LambdaTriggerTag")
                                if not LAMBDA_TRIGGER_TAG:
                                    elastio_lambda_arn = tag_list[LAMBDA_TRIGGER_TAG].lower()
                                else:
                                    raise Exception('ElastioImportLambdaARN is missing!')
                            
                            rt_plan_arn = event_detail.get('restoreTestingPlanArn')
                            if not rt_plan_arn:
                                logger.info(f'Response NOT part of restore testing (RT) feature. Validating whether its an RT Simulation Job : {restore_job_id}.')
                                #Look for awsb:rt-simulator:restore-job-id
                                restore_job_id_from_tag = tag_list.get('awsb:rt-simulator:restore-job-id')
                                if restore_job_id_from_tag:
                                    if restore_job_id_from_tag == restore_job_id:
                                        rt_plan_arn=f'rt-simulation-{restore_job_id}'
                                        #Remove the tag from the RP
                                        backup_client.untag_resource(ResourceArn=recovery_point_arn,TagKeyList=['awsb:rt-simulator:restore-job-id'])
                                        logger.info(f'Removed awsb:rt-simulator:restore-job-id from RP : {recovery_point_arn}')
                                    else:
                                        logger.warn(f'Response :{restore_job_id_from_tag} NOT part of the active Restore Job :{restore_job_id}. Skipping Elastio Workflow.')
                                        return                                        
                                else:
                                    logger.warn('Response NOT part of restore testing (RT) feature NOR RT Simulation. Skipping Elastio Workflow.')
                                    return

                            #invoke the lambda
                            input_params = {
                                                "aws_backup_vault": backup_vault_name,
                                                "aws_backup_rp_arn": recovery_point_arn,
                                                "elastio_vault": ELASTIO_DEFAULT_VAULT_NAME,
                                                "iscan": {
                                                            "ransomware": True,
                                                            "malware": True,
                                                            "entropy": False,
                                                            "send_event": True,
                                                            "event_bridge_bus": elastio_status_eb,
                                                            "user_data": f'{backup_vault_name},{enable_rp_fwd},{rt_plan_arn},{restore_job_id},{recovery_point_arn},{created_resource_arn}'
                                                        }
                                            }     
                    
                            if resource_type in ('EFS'):
                                restored_resource_id = get_efs_resource_id(created_resource_arn)
                                prep_for_efs_restore_test(restored_resource_id)
                            elif resource_type in ('S3'):
                                if ':::' in created_resource_arn:
                                    restored_resource_id = created_resource_arn.split(':::')[1]
                                    logger.info('S3 does NOT support Ransomware Scanning. Setting ransomware to False')
                                input_params["iscan"]["ransomware"] = False
                            elif resource_type == SUPPORTED_TYPE_FOR_SIMULATION:
                                #Handle tagging the created resource arn ebs volumes with appropriate tags
                                response = backup_client.get_recovery_point_restore_metadata(BackupVaultName=backup_vault_name,RecoveryPointArn=recovery_point_arn)
                                restore_meta_data = response.get('RestoreMetadata')
                                vm_disk_meta_data = restore_meta_data.get('disks')
                                if vm_disk_meta_data:
                                    restored_resource_id = created_resource_arn                          
                                    #Handle EBS/ EC2
                                    if -1 != created_resource_arn.find("/"):
                                        # Handle for EC2 and EBS
                                        restored_resource_id = created_resource_arn.split("/")[1]                           
                                        tag_ebs_volumes_with_vm_disk_meta_data(restored_resource_id,restore_job_id,vm_disk_meta_data)
                                    else:
                                        raise ValueError(f"Invalid ARN format for {created_resource_arn}")

                            else:
                                restored_resource_id = created_resource_arn                          
                                #Handle EBS/ EC2
                                if -1 != created_resource_arn.find("/"):
                                    # Handle for EC2 and EBS
                                    restored_resource_id = created_resource_arn.split("/")[1]                            
                            
                            logger.info(f'Processing restored_resource_id :{restored_resource_id} for Resource Type : {resource_type}')
                            #invoke the lambda
                            input_params["restored_resource_id"] = restored_resource_id                   

                            # Let the customer control whether we do a scan-only, ingest, or ingest-and-scan 
                            # of this RP.  If no `elastio:restore-test` tag is specified, we'll let the Elastio
                            # integration Lambda decide what the default behavior should be.    
                            #
                            # the odd for loop is needed because we want this tag to be case-insensitive to maximum
                            # customer convenience.
                            elastio_action = None
                            for tag in tag_list.keys():  # Iterate over the keys in tag_list
                                if tag.lower() == ENABLE_ELASTIO_SCAN_TAG:  # Case-insensitive comparison
                                    elastio_action = tag_list[tag].lower()  # Convert the value to lower case
                                    break  # Exit the loop once the tag is found
                            
                            if elastio_action:
                                if elastio_action not in ["scan", "ingest", "ingest-and-scan"]:
                                    logger.warning(f'Invalid action "{elastio_action}" specified for RP {recovery_point_arn}. Defaulting to "scan".')
                                    elastio_action = "scan"
                                
                                input_params['action'] = elastio_action
                                logger.info(f'Action specified for RP {recovery_point_arn} is {elastio_action}')
                            
                            logger.info(f'invoking {elastio_lambda_arn} with {input_params}')
                            try:
                                response = boto3.client('lambda').invoke(
                                    FunctionName=elastio_lambda_arn,
                                    InvocationType='Event',
                                    Payload=json.dumps(input_params)
                                )
                                logger.info(f'Invoked {elastio_lambda_arn} with {input_params} - response : {response}')

                            except (ClientError, Exception):  # pylint: disable = W0703
                                var = traceback.format_exc()
                                logger.error(f"Error {var} processing invoke for elastio iscan lambda")    
                        else:
                            logger.info(f"Elastio scanning NOT enabled for RP via tag_list : {tag_list} and ENABLE_ELASTIO_SCAN_TAG :{ENABLE_ELASTIO_SCAN_TAG}")    


                except Exception:
                    var = traceback.format_exc()
                    logger.error(f"Error {var} in handle_backup_event")

            def tag_ebs_volumes_with_vm_disk_meta_data(instance_id, restore_job_id,new_tags):

                try:
                    new_tags = json.loads(new_tags)
                    logger.info(f'tag_ebs_volumes_with_vm_disk_meta_data with {new_tags}')    
                    ec2_client = boto3.client('ec2')

                    # Describe the instance to get current tags
                    response = ec2_client.describe_instances(InstanceIds=[instance_id])
                    
                    # Extract current tags
                    current_tags = {}
                    if 'Tags' in response['Reservations'][0]['Instances'][0]:
                        current_tags = {tag['Key']: tag['Value'] for tag in response['Reservations'][0]['Instances'][0]['Tags']}

                    ec2_tags = {'awsbackup-restore-test': restore_job_id}
                    logger.info(f'New Tags to set for EC2: {ec2_tags}')    

                    # Merge new tags with existing tags
                    for key, value in ec2_tags.items():
                        current_tags[key] = value
                
                    # Convert tags back to the format required by create_tags
                    merged_tags = [{'Key': key, 'Value': value} for key, value in current_tags.items()]
                    logger.info(f'Tags to set : {merged_tags}')
                      
                    
                    # Update tags on the instance
                    ec2_client.create_tags(Resources=[instance_id],Tags=merged_tags)

                    response = ec2_client.describe_volumes(
                        Filters=[{'Name': 'attachment.instance-id', 'Values': [instance_id]}]
                    )
                    volumes = response['Volumes']
                    # Iterate over volumes and add new tags
                    for idx, volume in enumerate(volumes):
                        volume_id = volume['VolumeId']
                        logger.info(f"Processing volume: {volume_id} at index {idx}")

                        # Append index to each tag key
                        tag_to_set = new_tags[idx]
                        logger.info(f'tag_to_set for {volume_id} is {tag_to_set}')
                        new_tags_to_set = {'awsb:disk-id': tag_to_set['diskId'],'awsb:disk-label': tag_to_set['label']}
                        logger.info(f'New Tags : {new_tags_to_set}')

                        # Get the current tags of the volume
                        response = ec2_client.describe_tags(
                            Filters=[{'Name': 'resource-id', 'Values': [volume_id]}]
                        )
                        
                        # Extract existing tags
                        existing_tags = {tag['Key']: tag['Value'] for tag in response['Tags']}
                    
                        # Merge new tags with existing tags
                        for key, value in new_tags_to_set.items():
                            existing_tags[key] = value
                    
                        # Convert tags back to the format required by create_tags
                        merged_tags = [{'Key': key, 'Value': value} for key, value in existing_tags.items()]
                        logger.info(f'Tags to set : {merged_tags}')

                        # Apply the updated tags to the volume
                        ec2_client.create_tags(
                            Resources=[volume_id],
                            Tags=merged_tags
                        )
                except Exception:
                    var = traceback.format_exc()
                    logger.error(f"Error {var} in tag_ebs_volumes_with_vm_disk_meta_data")


            def save_event_data_to_s3(s3_log_bucket,json_content):
                """
                This method is responsible for writing the json_content to the s3_log_bucket
                """
                try:
                    job_id = json_content.get('job_id') 
                    s3_log_location = 'elastio-scan-results/' + job_id + '.json'        
                    logger.info(f"Persisting event data to : {s3_log_bucket} at {s3_log_location}")
                    s3_client = boto3.client('s3')
                    s3_client.put_object(Body=json.dumps(json_content,
                                                    default=str, separators=(',', ':')),
                                    Bucket=s3_log_bucket,
                                    Key=s3_log_location, ACL='bucket-owner-full-control',
                                    Tagging='Source=ElastioResults')
                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} processing save_event_data_to_s3") 
                
            def process_ransomware_details(account_id,product_arn,generator_id,scan_timestamp,aws_asset_id,aws_backup_rp_arn,elastio_rp_id,ransomware_details):
                """
                This is the function responsbile to create ransomware findings based on ransomware_details
                """      
                try:
                    logger.info('Starting process_ransomware_details')
                    title = 'Ransomware scan results'
                    ransomware_report = ransomware_details['report']
                    if ransomware_report:
                        ransomware_encrypted_files = ransomware_report.get('encrypted_files')
                        if ransomware_encrypted_files:
                            ransomware_state = 'OBSERVED'
                            for ransomware_encrypted_file in ransomware_encrypted_files:
                                parent_directory = ransomware_encrypted_file.get('parent_directory')
                                filename = ransomware_encrypted_file.get('filename')
                                file_path = f'{parent_directory}/{filename}'
                                suspected_ransomware = ransomware_encrypted_file.get('suspected_ransomware')
                                suspected_ransomware_names = [suspected_ransomware_item['name'] for suspected_ransomware_item in suspected_ransomware]
                                
                                # Convert the list of names into a comma-separated string
                                suspected_ransomware_names = ', '.join(suspected_ransomware_names)
                                scan_result_type = 'RANSOMWARE'
                                
                                ransomware_obj = { 
                                                'Name': suspected_ransomware_names,
                                                'Path': file_path,
                                                'State': ransomware_state,
                                                'Type': scan_result_type
                                            }
                                import_security_hub_findings(account_id,product_arn,generator_id,scan_timestamp,title,aws_asset_id,aws_backup_rp_arn,elastio_rp_id,ransomware_obj)
                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} processing process_ransomware_details")
                
                    
            def process_malware_details(account_id,product_arn,generator_id,scan_timestamp,aws_asset_id,aws_backup_rp_arn,elastio_rp_id,malware_details):
                """
                This is the function responsbile to create malware findings based on malware_details
                """        
                try:
                    logger.info('Starting process_malware_details')
                    title = 'Malware scan results'
                    malware_report = malware_details['report']
                    if malware_report:
                        malware_suspicious_files = malware_report.get('suspicious_files')
                        if malware_suspicious_files:
                            malware_state = 'OBSERVED'
                            for malware_suspicious_file in malware_suspicious_files:
                                file_path = malware_suspicious_file.get('path')
                                scan_result = malware_suspicious_file.get('scan_result')
                                malware_name = 'EncryptedFile'
                                scan_result_type = 'BLENDED_THREAT' 
                                if isinstance(scan_result, dict):
                                    infected_data = scan_result.get('Infected')
                                    if infected_data:
                                        scan_result_type =  infected_data.get('threat_type')
                                        if scan_result_type and scan_result_type == 'Virus':
                                            scan_result_type = 'VIRUS'
                                        malware_name = infected_data.get('threat_name')
                                malware_obj = { 
                                                'Name': malware_name,
                                                'Path': file_path,
                                                'State': malware_state,
                                                'Type': scan_result_type
                                            }
                                
                                import_security_hub_findings(account_id,product_arn,generator_id,scan_timestamp,title,aws_asset_id,aws_backup_rp_arn,elastio_rp_id,malware_obj)
                                
                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} processing process_malware_details")      
                

            def update_rp_validation_status(event):
                """
                This is the main function that will process the elastio response JSON to update RT Status
                """ 
                try:
                    logger.info('Starting update_rp_validation_status')
                    event_details = event['detail']
                    elastio_scan_reports = event_details.get('reports')
                    for elastio_report in elastio_scan_reports:
                        logger.info(f'Processing elastio_report : {elastio_report}')
                        scan_summary = elastio_report.get('summary')
                        user_data = elastio_report.get('user_data')
                        aws_backup_rp_arn = elastio_report.get('aws_backup_rp_arn')                        
                        rt_plan_arn = None
                        restore_job_id = None
                        enable_rp_fwd = False
                        if user_data:
                            logger.info(f'user_data : {user_data}')
                            user_data_list = user_data.split(',')
                            #"user_data": f'{backup_vault_name},{enable_rp_fwd},{rt_plan_arn},{restore_job_id},{recovery_point_arn},{created_resource_arn}'
                            if len(user_data_list) > 0:
                                backup_vault_name=user_data_list[0]
                            if len(user_data_list) > 1:
                                enable_rp_fwd=bool(user_data_list[1])                                
                            if len(user_data_list) > 2:
                                rt_plan_arn = user_data_list[2]
                            if len(user_data_list) > 3:                             
                                restore_job_id = user_data_list[3]

                        aws_asset_id = scan_summary.get('aws_asset_id')
                        

                        is_rp_clean = scan_summary.get('clean')
                        summary_details = elastio_report.get('summary')
                        status_message = 'Recovery point is clean of Malware and Ransomware infection.'
                        status = 'SUCCESSFUL'

                        if summary_details:
                            if not is_rp_clean:
                                status = 'FAILED'
                                ransomware_files_detected = summary_details.get("ransomware_files_detected")                    
                                malware_files_detected = summary_details.get("malware_files_detected")
                                status_message = f'Recovery point is compromised with {malware_files_detected} Malware and {ransomware_files_detected} Ransomware infection(s).'
                            else:
                                if enable_rp_fwd:
                                    #Forward the original recovery point to a safe vault once the scan is clean
                                    if backup_vault_name and AWSB_COPY_IAM_ROLE and ELASTIO_SCAN_AFTER_COPY_DESTINATION:
                                        forward_recovery_point_to_vault(backup_vault_name,aws_backup_rp_arn)
                                    else:
                                        logger.error('AWS Backup Vault name is missing in Elastio User Data. Skipping copy Recovery Point.')
                                else:
                                    logger.info(f'AWS Backup RP : {aws_backup_rp_arn} is NOT marked for Forwarding. Skipping Copy')
                        #Update RP Status
                        try:
                            logger.info(f'attempting update_rt_job_status for restore_job_id :{restore_job_id}, rt_plan_arn :{rt_plan_arn},aws_asset_id :{aws_asset_id}, status : {status}, status_message :{status_message} ')
                            update_rt_job_status(restore_job_id,rt_plan_arn,status,status_message)
                            
                        except (ClientError, Exception):
                            var = traceback.format_exc()
                            logger.error(f"Error {var} processing update_rt_job_status")            
                        

                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} processing update_rp_validation_status")   


            def generate_security_hub_findings(event):
                """
                This is the main function that will process the elastio response JSON to create
                SecurityHub findings
                """ 
                try:
                    logger.info('Starting generate_security_hub_findings')
                    awsRegion = event['region']
                    account_id = event['account']
                    generator_id = f"{account_id}/elastio.iscan/{event.get('job_id')}"
                    product_arn = 'arn:aws:securityhub:' + awsRegion + ':' + account_id + ':product/' + account_id + '/default'    
                    event_details = event['detail']
                    scan_timestamp = event['time']
                    elastio_scan_reports = event_details.get('reports')
                    for elastio_report in elastio_scan_reports:
                        scan_summary = elastio_report.get('summary')
                        aws_backup_rp_arn = elastio_report.get('aws_backup_rp_arn')
                        aws_asset_id = scan_summary.get('aws_asset_id')

                        # When scanning an Elastio RP, this is a string with the RP ID and the Elastio asset ID separated by `:`, eg
                        # rp-01hb8zhddexp111v2wf6zqnx6s:elastio:asset:aws-ebs:s:968455818835:us-east-2:vol-09678ed07cb34fb40
                        #
                        # When performing scan-only on an AWS Backup RP, the format is $aws_rp_arn:$snapshot_id, eg:
                        # aws:ec2:us-east-2::snapshot/snap-09205755f3e7984c7:snap-09205755f3e7984c7
                        #
                        # Since we already know the backup ARN from a dedicated report field, and don't actually care about
                        # the individual EBS snapshot that was scanned, we'll ignore the cases where there is no RP ID for the
                        # purposes of generating a SecurityHub alert.
                        elastio_asset = elastio_report.get('asset')
                        
                        if elastio_asset.startswith("rp-") and ":" in elastio_asset:
                            elastio_rp_id = elastio_asset.split(":", 1)[0]  # split at the first occurrence of ":" and return the first part
                        else:
                            elastio_rp_id = None

                        is_rp_clean = scan_summary.get('clean')
                        malware_details = elastio_report.get('malware')
                        ransomware_details = elastio_report.get('ransomware')
                        summary_details = elastio_report.get('summary')
                        user_data = elastio_report.get('user_data')
                        aws_backup_vault_name = None
                        enable_rp_fwd = False
                        if user_data:
                            user_data = user_data.strip()
                            user_data_list = user_data.split(',')
                            logger.info(f'user_data : {user_data}')
                            if len(user_data_list) > 0:
                                aws_backup_vault_name=user_data_list[0]
                            if len(user_data_list) > 1:
                                enable_rp_fwd=bool(user_data_list[1])
                        if not is_rp_clean:
                            process_malware_details(account_id,product_arn,generator_id,scan_timestamp,aws_asset_id,aws_backup_rp_arn,elastio_rp_id, malware_details)
                            process_ransomware_details(account_id,product_arn,generator_id,scan_timestamp,aws_asset_id,aws_backup_rp_arn,elastio_rp_id, ransomware_details)
                            create_summary_insights(aws_backup_rp_arn,summary_details)
              
                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} processing generate_security_hub_findings")   

            def forward_recovery_point_to_vault(aws_backup_vault_name, aws_backup_rp_arn):
                """
                Copy the clean AWS Backup recovery point to the user defined vault.

                :param aws_backup_vault_name: Name of the source AWS Backup vault
                :param aws_backup_rp_arn: ARN of the recovery point to be copied
                :return: True if the copy job was started successfully, False otherwise
                """
                if not all([ELASTIO_SCAN_AFTER_COPY_DESTINATION, AWSB_COPY_IAM_ROLE]):
                    logger.error("Missing required environment variables for copy operation")
                    return False

                try:
                    logger.info(f'Copying clean Recovery Point: {aws_backup_rp_arn} '
                                f'from {aws_backup_vault_name} '
                                f'to {ELASTIO_SCAN_AFTER_COPY_DESTINATION} '
                                f'using role: {AWSB_COPY_IAM_ROLE}')
                    
                    response = backup_client.start_copy_job(
                        RecoveryPointArn=aws_backup_rp_arn,
                        SourceBackupVaultName=aws_backup_vault_name,
                        DestinationBackupVaultArn=ELASTIO_SCAN_AFTER_COPY_DESTINATION,
                        IamRoleArn=AWSB_COPY_IAM_ROLE
                    )
                    
                    logger.info(f'Copy job started successfully: {response}')
                    return True

                except ClientError as e:
                    error_code = e.response['Error']['Code']
                    error_message = e.response['Error']['Message']
                    logger.error(f"AWS API error in forward_recovery_point_to_vault: {error_code} - {error_message}")
                except Exception as e:
                    logger.error(f"Unexpected error in forward_recovery_point_to_vault: {str(e)}")
                
                return False
      
                    
            def import_security_hub_findings(account_id,product_arn,generator_id,scan_timestamp,title,aws_asset_id,aws_backup_rp_arn,elastio_rp_id,malware_obj):   
                """
                This function is responsible for creating a finding in SecurityHub based on malware_obj
                """        
                try:
                    logger.info('Starting import_security_hub_findings')
                    finding_id = f"{account_id}/{uuid.uuid4()}"
                    description = f'Details of {title} for AWS Backup Recovery Point : {aws_backup_rp_arn}'
                    sechub_finding = {
                                'SchemaVersion': '2018-10-08', 
                                'Id': finding_id,
                                'ProductArn': product_arn,
                                'GeneratorId': generator_id,
                                'AwsAccountId': account_id,
                                'Types': [ 'Malware and Ransomware Scans' ],
                                'FirstObservedAt': scan_timestamp,
                                'UpdatedAt': scan_timestamp,
                                'CreatedAt': scan_timestamp,
                                'Severity': {
                                'Label': "HIGH"
                                },
                                'Title': title,
                                'Description': description,
                                'ProductName':'Elastio iScan',
                                'CompanyName':'Elastio',
                                'WorkflowState': 'NEW', 
                                'Compliance': {'Status': 'FAILED'},                    
                                'Resources': [
                                {
                                    'Id': aws_asset_id,
                                    'Type': "VolumeId"
                                },
                                {
                                    'Id': aws_backup_rp_arn,
                                    'Type': "RecoveryPointId"
                                },
                                ],
                                'Malware': [malware_obj]
                            }
                    
                    # Some customers only look at SecurityHub, they don't have the optional observability
                    # component enabled, so for now the only notice they will get of an adverse finding on the AWS Backup RP
                    # will be via SecurityHub.  So it's important that we include the Elastio RP ID so they are able to find
                    # it in the UI.
                    if elastio_rp_id is not None:
                        sechub_finding['Resources'].append({
                            'Id': elastio_rp_id,  

                            # Per the AWS Security Hub Security Finding Format (https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-findings-format-syntax.html)
                            # non-AWS IDs must have type "Other"
                            'Type': 'Other'
                        })

                    logger.info(f'batch_import_findings with finding : {sechub_finding}')
                    response = sechub_client.batch_import_findings(Findings=[sechub_finding])
                    successCount=response['SuccessCount']
                    failedCount=response['FailedCount']
                    failedFindings=response['FailedFindings']
                    
                    logger.info(f'batch_import_findings finished. FailedCount: {failedCount},SuccessCount : {successCount}. FailedFindings: {failedFindings}')
                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} processing import_security_hub_findings") 

            def create_summary_insights(awsbackup_rp_id, summary_details):
                """
                This function is responsible to create an insight based on the findings
                created for the awsbackup_rp_id
                """    
                
                try:
                    logger.info(f'Starting create_summary_insights with {summary_details}')
                    aws_asset_id = summary_details.get('aws_asset_id')
                    insight_name = f'Elastio Scan results for resource : {aws_asset_id} using RP : {awsbackup_rp_id}'
                    if len(insight_name) > 128:
                        insight_name = insight_name[:125] + '...'
                    filters = {
                        'ResourceId': [{
                            'Value': awsbackup_rp_id,
                            'Comparison': 'EQUALS'
                        }],
                        'ResourceType': [{
                            'Value': 'RecoveryPointId',
                            'Comparison': 'EQUALS'
                        }],
                        'CompanyName': [{
                            'Value': 'elastio.com',
                            'Comparison': 'EQUALS'
                        }],
                        'WorkflowStatus': [{
                            'Value': 'NEW',
                            'Comparison': 'EQUALS'
                        }]
                    }    
                    
                    group_by_attribute = 'ResourceType'     
                    logger.info(f'create_insight :{insight_name} with filter : {filters} and group_by_attribute : {group_by_attribute}')
                    
                    response = sechub_client.create_insight(
                        Name=insight_name,
                        Filters=filters,
                        GroupByAttribute=group_by_attribute
                    )
                    insight_arn = response['InsightArn']
                    logger.info(f'Insight : {insight_arn} created successfully')
                    
                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} processing create_summary_insights")    

            def send_event_to_global_event_bus(event_type, json_content):
                """
                Send an event to the global event bus.

                :param event_type: Type of the event
                :param json_content: JSON content to be sent as the event detail
                :return: True if the event was sent successfully, False otherwise
                """
                global_event_bus_arn = os.environ.get("AWSBackupEventsBusArn")
                
                if not global_event_bus_arn:
                    logger.warning("AWSBackupEventsBusArn not set in environment variables")
                    return False

                try:
                    json_payload = json.dumps(json_content, default=str, separators=(",", ":"))
                    
                    response = events_client.put_events(
                        Entries=[
                            {
                                "Time": datetime.now(timezone.utc),
                                "Source": "observer.events",
                                "DetailType": event_type,
                                "Detail": json_payload,
                                "EventBusName": global_event_bus_arn,
                            }
                        ]
                    )
                    
                    failed_entry_count = response.get('FailedEntryCount', 0)
                    if failed_entry_count > 0:
                        logger.error(f"Failed to send {failed_entry_count} entries to the event bus")
                        return False
                    
                    logger.info(f"Successfully sent event of type {event_type} to {global_event_bus_arn}")
                    return True

                except ClientError as e:
                    logger.error(f"AWS API error in send_event_to_global_event_bus: {e}")
                except json.JSONDecodeError as e:
                    logger.error(f"JSON encoding error: {e}")
                except Exception as e:
                    logger.error(f"Unexpected error in send_event_to_global_event_bus: {str(e)}")
                
                return False
   
                
            def handle_elastio_iscan_event(scan_event_data):
                logger.info(
                    f"Handling event for elastio.iscan events, payload :{scan_event_data}"
                )

                try:
                    # Log the Combined Event
                    job_event_type = "scan_results"
                    enable_rp_fwd=False
                    scan_event_data["job_type"] = job_event_type
                    enable_observer_integration = os.environ.get('EnableObserverIntegration')
                    scan_event_data['job_id']=scan_event_data.get('id')
                    if enable_observer_integration.lower() in ['true', '1', 't', 'y','yes']:            
                        #Check if Observer Integration exists, if so sent results to Observer
                        send_event_to_global_event_bus(job_event_type, scan_event_data)

                    s3_log_bucket = os.environ.get('LogsBucketName') 
                    if s3_log_bucket:
                        save_event_data_to_s3(s3_log_bucket,scan_event_data)
                    else:
                        logger.info('S3 Log Bucket Name Env Paramter LogsBucketName is missing. Skipping logging to S3 Bucket')
                    event_details = scan_event_data['detail']
                    elastio_scan_reports = event_details.get('reports')
                    if elastio_scan_reports:
                        enable_sechub_integration = os.environ.get('EnableSHIntegration')
                        if not enable_sechub_integration:
                            enable_sechub_integration = 'True' 

                        if enable_sechub_integration.lower() in ['true', '1', 't', 'y','yes']:
                            #Generate SH Findings
                            generate_security_hub_findings(scan_event_data)               
                        
                        #update the RP Validation Status
                        update_rp_validation_status(scan_event_data)
                    else:
                        user_data = event_details.get('user_data')
                        rt_plan_arn = None
                        restore_job_id = None
                        created_resource_arn = None
                        aws_backup_rp_arn = None
                        #"user_data": f'{backup_vault_name},{enable_rp_fwd},{rt_plan_arn},{restore_job_id},{recovery_point_arn},{created_resource_arn}'
                        if user_data:
                            logger.info(f'user_data : {user_data}')
                            user_data_list = user_data.split(',')
                            if len(user_data_list) > 0:
                                backup_vault_name=user_data_list[0]
                            if len(user_data_list) > 1:
                                enable_rp_fwd=bool(user_data_list[1])
                            if len(user_data_list) > 2:
                                rt_plan_arn = user_data_list[2]
                            if len(user_data_list) > 3:                                
                                restore_job_id = user_data_list[3] 
                            if len(user_data_list) > 4:
                                aws_backup_rp_arn = user_data_list[4]                                   
                            if len(user_data_list) > 5:
                                created_resource_arn = user_data_list[5]                                            
                        status_message = event_details.get('error_message') 
                        logger.error(f'Elastio scan failed for {rt_plan_arn} :{restore_job_id} with error : {status_message} ')
                        status = 'FAILED'
                        if restore_job_id:
                            update_rt_job_status(restore_job_id,rt_plan_arn,status,status_message)

                        if created_resource_arn and rt_plan_arn:
                            if rt_plan_arn.startswith('rt-simulation'):
                                restored_resource_id = created_resource_arn.split("/")[1]
                                terminate_restore_tested_instance(restored_resource_id)
                except Exception as e:
                    logger.error(
                        f"Error : {e} processing handle_elastio_iscan_event"
                    )

            def terminate_restore_tested_instance(instance_id):
                """
                Terminate an EC2 instance.

                :param instance_id: ID of the EC2 instance to terminate
                :return: True if termination was successful, False otherwise
                """
                if not instance_id:
                    logger.error("No instance ID provided for termination")
                    return False

                try:
                    response = ec2_client.terminate_instances(InstanceIds=[instance_id])
                    terminating_instance = response['TerminatingInstances'][0]
                    current_state = terminating_instance['CurrentState']['Name']
                    logger.info(f"Termination initiated for instance {instance_id}. Current state: {current_state}")
                    return True

                except ClientError as e:
                    error_code = e.response['Error']['Code']
                    error_message = e.response['Error']['Message']
                    logger.error(f"Error terminating instance {instance_id}: {error_code} - {error_message}")
                except KeyError as e:
                    logger.error(f"Unexpected response format when terminating instance {instance_id}: {str(e)}")
                except Exception as e:
                    logger.error(f"Unexpected error when terminating instance {instance_id}: {str(e)}")
                
                return False

            def update_rt_job_status(restore_job_id, rt_plan_arn, status, status_message):
                """
                Update the status of a restore job in AWS Backup.

                :param restore_job_id: ID of the restore job
                :param rt_plan_arn: ARN of the restore test plan
                :param status: Status to be set for the restore job
                :param status_message: Message describing the status
                """
                logger.info(f'Updating status for {rt_plan_arn}: {restore_job_id} with status: {status} and message: {status_message}')

                # Truncate status message if it exceeds the maximum length
                status_message = status_message[:RT_STATUS_MESSAGE_MAX_LENGTH]

                try:
                    backup_client.put_restore_validation_result(
                        RestoreJobId=restore_job_id,
                        ValidationStatus=status,
                        ValidationStatusMessage=status_message
                    )
                    logger.info(f'Successfully updated status for {restore_job_id} to: {status}')
                except ClientError as e:
                    logger.error(f'Error updating restore job status: {e.response["Error"]["Message"]}')
                except Exception as e:
                    logger.error(f'Unexpected error updating restore job status: {str(e)}')                

            def handle_ec2_ami_state_change(event):
                detail_type = event.get("detail-type")
                event_detail = event.get("detail", {})
                job_event_state = event_detail.get("state") or event_detail.get("status")
                resources = event.get('resources', [])
                if detail_type == "EC2 AMI State Change":
                    recovery_point_arn = next((resource for resource in resources if ':backup-vault:' not in resource), None)
                    #https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitor-ami-events.html#ami-event-available
                    ec2_image_id = event_detail.get('ImageId')
                    logger.info(f'Handling EC2 AMI State Change event: {job_event_state}, {detail_type} for EC2 AMI Id: {ec2_image_id}')
                    aws_backup_vault_name = find_aws_backup_vault_name(recovery_point_arn)
                    if not aws_backup_vault_name:
                        logger.info(f'Handling non AWS Backup created AMI: {recovery_point_arn}')
                        tag_recovery_point_with_meta_data('EC2', recovery_point_arn)                    

            def lambda_handler(event, context):
                """
                Main Handler for Elastio Restore Simulator
                """
                logger.info(f'Processing Event: {event}, context: {context}')
                
                event_handlers = {
                    'aws.backup': handle_backup_event,
                    'aws.ec2': handle_ec2_ami_state_change,
                    'elastio.iscan': handle_elastio_iscan_event
                }
                
                try:
                    event_source = event.get("source")
                    handler = event_handlers.get(event_source)
                    
                    if handler:
                        handler(event)
                    else:
                        logger.warning(f"Unhandled event source: {event_source}")
                
                except ClientError as e:
                    logger.error(f"AWS API error in lambda_handler: {e}")
                except Exception as e:
                    logger.error(f"Unexpected error in lambda_handler: {str(e)}")
                    logger.error(f"Traceback: {traceback.format_exc()}")                    


  EC2AMICreationEventRule:
    Type: 'AWS::Events::Rule'
    Properties: 
      Description: 'Rule to direct EC2 AMI creation Events to ElastioHelper Lambda'
      EventPattern: 
        source: 
          - 'aws.ec2'
        detail-type: 
          - 'EC2 AMI State Change'
        detail: 
          State: 
            - 'available'
      Targets: 
        - Arn: !GetAtt "ElastioHelperLambda.Arn"
          Id: "ProcessEC2AMIStateChangeUsingLambda" 

  EC2AMICreationEventRuleInvokePermission:
    Type: 'AWS::Lambda::Permission'
    Properties:
      FunctionName: !Ref ElastioHelperLambda
      Action: 'lambda:InvokeFunction'
      Principal: 'events.amazonaws.com'
      SourceArn: !GetAtt EC2AMICreationEventRule.Arn

  ProcessAWSBEventForElastioHelper: 
    Type: AWS::Events::Rule
    Properties: 
      Name: ProcessAWSBEventForElastioHelper
      Description: "Rule to direct AWS Backup Events events to ElastioHelper Lambda"
      State: "ENABLED"
      EventPattern: 
        source:
          - 'aws.backup'
        detail-type:
          - 'Recovery Point State Change'
          - 'Restore Job State Change'     
        detail:
          status:
            - COMPLETED               
      Targets: 
        - Arn: !GetAtt "ElastioHelperLambda.Arn"
          Id: "ProcessAWSBackupEventsUsingLambda" 
          
  ProcessAWSBEventForElastioHelperInvokePermission:
    Type: 'AWS::Lambda::Permission'
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !Ref ElastioHelperLambda
      Principal: events.amazonaws.com
      SourceArn: !Sub ${ProcessAWSBEventForElastioHelper.Arn}

  ElastioHelperEventBus:
    Type: AWS::Events::EventBus
    Properties:
      Name: !Join [ '', ['ElastioHelperEventBus-', !Ref 'AWS::AccountId','-', !Ref 'AWS::Region'] ]
      Tags:
        - Key: !Ref ElastioTagKey
          Value: !Ref ElastioTagValue

  ElastioStatusEventBridgeInvokePermission:
    Type: 'AWS::Lambda::Permission'
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !Ref ElastioHelperLambda
      Principal: events.amazonaws.com
      SourceArn: !Sub ${ElastioStatusEventRuleForRTSimulator.Arn}    
  
  ElastioStatusEventRuleForRTSimulator: 
    Type: AWS::Events::Rule
    Properties: 
      Description: "EventBridge rule to send Elastio events to ElastioHelper Lambda"
      EventBusName: !Ref ElastioHelperEventBus
      State: "ENABLED"
      EventPattern: 
        source:
          - 'elastio.iscan'
      Targets: 
        - Arn: !GetAtt ElastioHelperLambda.Arn
          Id: "ElastioStatusEvent"
  
  ElastioHelperEventBusPolicy:
    Type: AWS::Events::EventBusPolicy
    Properties: 
        EventBusName: !Ref ElastioHelperEventBus
        StatementId: "ElastioHelperEventBusPolicyStmt"
        Statement: 
            Effect: "Allow"
            Principal: "*"
            Action: "events:PutEvents"
            Resource: !GetAtt "ElastioHelperEventBus.Arn"

  #TODO Move this to another stack and deploy it on demand
  ElastioHelperLambdaRole:
    Type: 'AWS::IAM::Role'
    Metadata:    
      cfn_nag:
        rules_to_suppress:
          - id: F3
          - id: W11
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      RoleName: 'ElastioHelperLambdaRole'
      Description: 'IAM Role to allow permissions for the AWS Backup Elastio Helper Lambda'            
      ManagedPolicyArns:
          - !Sub 'arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
      Policies:        
          - PolicyName: invokeAndPassRoleForLambda
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
              - Effect: Allow
                Action:
                - lambda:InvokeFunction
                Resource: '*'     
              - Effect: Allow
                Action:
                - iam:PassRole
                Resource: '*'                       
          - PolicyName: s3LogsBucketPermissions
            PolicyDocument:
              Statement:
              - Effect: Allow
                Action:
                  - kms:GenerateDataKey
                  - kms:Decrypt
                  - kms:Encrypt                  
                  - s3:PutObject*
                  - s3:GetObject*
                  - s3:DeleteObject
                  - s3:*BucketNotification
                  - s3:GetBucketLocation
                  - s3:ListBucket
                  - s3:ListBucketMultipartUploads
                  - s3:ListMultipartUploadParts
                  - s3:AbortMultipartUpload                  
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${LogsBucketName}/*'
                  - !Sub 'arn:${AWS::Partition}:s3:::${LogsBucketName}'
          - PolicyName: logStreamPermissions
            PolicyDocument:
              Statement:                       
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: !Sub 'arn:${AWS::Partition}:logs:*:*:*'   
          - PolicyName: backupPermissions
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
              - Effect: Allow
                Action:
                - backup:ListTags
                - backup:TagResource
                - backup:UntagResource
                - backup:DescribeRecoveryPoint                
                - backup:DescribeRecoveryPoint
                - backup:GetRecoveryPointRestoreMetadata     
                - backup:StartRestoreJob
                - backup:StartCopyJob
                - backup:DescribeRestoreJob
                - backup:ListBackupVaults 
                - backup:ListRecoveryPointsByBackupVault
                - backup:PutRestoreValidationResult
                Resource: '*'      
          - PolicyName: ec2Permissions
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
              - Effect: Allow
                Action:
                - ec2:DescribeImages
                - ec2:DescribeVolumes
                - ec2:DescribeInstances
                - ec2:DescribeSnapshots
                - ec2:CreateSecurityGroup
                - ec2:AuthorizeSecurityGroupIngress
                - ec2:DescribeSecurityGroups
                - ec2:DescribeVpcs
                - ec2:DescribeSubnets
                - ec2:DescribeTags
                - ec2:CreateTags
                - ec2:TerminateInstances
                - ec2:ModifyVpcAttribute
                Resource: '*'   
          - PolicyName: efsPermissions
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
              - Effect: Allow
                Action:
                - elasticfilesystem:CreateMountTarget
                - elasticfilesystem:DescribeMountTargetSecurityGroups
                Resource: '*'                    
          - PolicyName: secHubPermissions
            PolicyDocument:
              Statement:                       
              - Effect: Allow
                Action:
                  - 'securityhub:BatchImportFindings'
                  - 'securityhub:CreateInsight'
                Resource: 
                  - !Sub 'arn:${AWS::Partition}:securityhub:*:${AWS::AccountId}:product/*/*'
                  - !Sub 'arn:${AWS::Partition}:securityhub:*:${AWS::AccountId}:hub/default'        
Outputs:
  StackName:
    Value: !Ref AWS::StackName                  
